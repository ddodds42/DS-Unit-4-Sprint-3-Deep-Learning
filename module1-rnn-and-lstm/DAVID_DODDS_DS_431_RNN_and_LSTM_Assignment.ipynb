{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "DAVID_DODDS_DS_431_RNN_and_LSTM_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ddodds42/DS-Unit-4-Sprint-3-Deep-Learning/blob/master/module1-rnn-and-lstm/DAVID_DODDS_DS_431_RNN_and_LSTM_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8IS5d9FY7LH",
        "colab_type": "text"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "## *Data Science Unit 4 Sprint 3 Assignment 1*\n",
        "\n",
        "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
        "\n",
        "![Monkey at a typewriter](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Chimpanzee_seated_at_typewriter.jpg/603px-Chimpanzee_seated_at_typewriter.jpg)\n",
        "\n",
        "It is said that [infinite monkeys typing for an infinite amount of time](https://en.wikipedia.org/wiki/Infinite_monkey_theorem) will eventually type, among other things, the complete works of Wiliam Shakespeare. Let's see if we can get there a bit faster, with the power of Recurrent Neural Networks and LSTM.\n",
        "\n",
        "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
        "\n",
        "- [ ] Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
        "\n",
        "- [ ] Then, use that trained RNN to generate Shakespearean-ish text.\n",
        "\n",
        "- [ ] Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
        "\n",
        "- [ ] Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running.\n",
        "\n",
        "- [ ] Then, once you've got a proof of concept - start pushing it more!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ltj1je1fp5rO",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from tensorflow.keras.callbacks import LambdaCallback, EarlyStopping\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQUJn_fuMq6m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFGTdSP-ySx6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHjIkdLux9Ir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/ddodds42/DS-Unit-4-Sprint-3-Deep-Learning/master/module1-rnn-and-lstm/spearshake.txt'\n",
        "\n",
        "r = requests.get(url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOKo_U6nQqZp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "4d593a8a-bf78-495e-8ac0-02bd1dd19779"
      },
      "source": [
        "spearshake = r.text\n",
        "print(spearshake[:100])"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r\n",
            "Contents\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "               THE SONNETS\r\n",
            "\r\n",
            "               ALL’S WELL THAT ENDS WELL\r\n",
            "\r\n",
            "        \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHmx1u3CBX-Y",
        "colab_type": "code",
        "outputId": "d1447fe1-bfe6-4640-fb96-d3552d053d7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "type(spearshake)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOkb-EAv32Kc",
        "colab_type": "code",
        "outputId": "ae65f7b5-5a7f-4535-a19e-afa4863688a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "spearshake0 = spearshake.replace('\\n',' ')\n",
        "spearshake0 = spearshake0.replace('\\r',' ')\n",
        "spearshake0[:100]"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'  Contents                       THE SONNETS                   ALL’S WELL THAT ENDS WELL            '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xFNo612L5oS",
        "colab_type": "code",
        "outputId": "76e3a2f4-a3ba-4bef-ab9a-d39d43d7c275",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "spearshake0[:1000]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Contents                   THE SONNETS                 ALL’S WELL THAT ENDS WELL                 THE TRAGEDY OF ANTONY AND CLEOPATRA                 AS YOU LIKE IT                 THE COMEDY OF ERRORS                 THE TRAGEDY OF CORIOLANUS                 CYMBELINE                 THE TRAGEDY OF HAMLET, PRINCE OF DENMARK                 THE FIRST PART OF KING HENRY THE FOURTH                 THE SECOND PART OF KING HENRY THE FOURTH                 THE LIFE OF KING HENRY THE FIFTH                 THE FIRST PART OF HENRY THE SIXTH                 THE SECOND PART OF KING HENRY THE SIXTH                 THE THIRD PART OF KING HENRY THE SIXTH                 KING HENRY THE EIGHTH                 KING JOHN                 THE TRAGEDY OF JULIUS CAESAR                 THE TRAGEDY OF KING LEAR                 LOVE’S LABOUR’S LOST                 THE TRAGEDY OF MACBETH                 MEASURE FOR MEASURE                 THE MERCHANT OF VENICE                 THE MERRY WIVES OF WINDSOR       '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pzpn0nJ22Ca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars = list(set(spearshake0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibHkwJ7HyJ6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "char_int = {c:i for i, c in enumerate(chars)}\n",
        "int_char = {i:c for i, c in enumerate(chars)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbMD1duVAg6D",
        "colab_type": "code",
        "outputId": "3d333461-5bfc-4403-f2fe-9ca75bd04e4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(chars)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eTbnHGP4sVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxlen = 40\n",
        "step = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dv8DPEb7HZVP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded = [char_int[c] for c in spearshake0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTE35eO0HnWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = []\n",
        "next_char = []\n",
        "\n",
        "for i in range(0, len(encoded) - maxlen, step):\n",
        "  sequences.append(encoded[i : i+maxlen])\n",
        "  next_char.append(encoded[i+maxlen])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnYu12u-JMov",
        "colab_type": "code",
        "outputId": "09140a55-9457-4dd7-a0ab-79951bbab8c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(sequences)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1110221"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qm1ZVEFtJQfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eaQveorJ5Ca",
        "colab_type": "code",
        "outputId": "f93e6784-88a1-44cd-9500-1bc134425462",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X.shape, y.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1110221, 40, 100), (1110221, 100))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTWu08vsJ-gE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, sequence in enumerate(sequences):\n",
        "  for t, char in enumerate(sequence):\n",
        "    X[i,t,char] = 1\n",
        "  \n",
        "  y[i, next_char[i]] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnDvFBTpKkon",
        "colab_type": "code",
        "outputId": "c8092804-3a9f-4640-854f-d0ade1e70888",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X.shape, y.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1110221, 40, 100), (1110221, 100))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeUlzwyFKqbJ",
        "colab_type": "code",
        "outputId": "ee654250-a8af-4b9e-9425-352a1c1c5535",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(X[0,0])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj1IssBtK2g9",
        "colab_type": "code",
        "outputId": "771a18f1-22eb-4d5a-93d2-8d7ba3ef4fc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "X[0,0]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False,  True, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca1IhVLYK5z0",
        "colab_type": "code",
        "outputId": "bfb96c8d-acd0-428c-e510-c7663f7a230f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(y[0])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytMjckM9LAK2",
        "colab_type": "code",
        "outputId": "8cc76ee3-4a9f-4aa3-fd97-51e722eedf7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "y[0]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False,  True, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "       False])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "405Llv5HLCVE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
        "model.add(Dense(len(chars), activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='nadam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5XDnI1DMASG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(preds):\n",
        "  preds = np.asarray(preds).astype('float64')\n",
        "  preds = np.log(preds) / 1\n",
        "  exp_preds = np.exp(preds)\n",
        "  preds = exp_preds / np.sum(exp_preds)\n",
        "  probas = np.random.multinomial(1, preds, 1)\n",
        "  return np.argmax(probas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtPYKhxiN1LX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def on_epoch_end(epoch, _):\n",
        "  print()\n",
        "  print('==== Generating text after Epoch %d' % epoch)\n",
        "  start_index = random.randint(0, len(spearshake0) - maxlen - 1)\n",
        "  generated = ''\n",
        "  sentence = spearshake0[start_index: start_index+maxlen]\n",
        "  generated += sentence\n",
        "  print('==== Generating with seed: \"'+ sentence +'\"')\n",
        "  sys.stdout.write(generated)\n",
        "  for i in range(400):\n",
        "    X_pred = np.zeros((1, maxlen, len(chars)))\n",
        "    for t, char in enumerate(sentence):\n",
        "      X_pred[0, t, char_int[char]] = 1\n",
        "    preds = model.predict(X_pred, verbose=0)[0]\n",
        "    next_index = sample(preds)\n",
        "    next_char = int_char[next_index]\n",
        "    sentence = sentence[1:]+next_char\n",
        "    sys.stdout.write(next_char)\n",
        "    sys.stdout.flush()\n",
        "  print()\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpwQeRMORM5P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "698c84c3-f142-4976-f0a0-caf14faa70d6"
      },
      "source": [
        "model.fit(\n",
        "    X, y, batch_size=256, epochs=10, #validation_split = 0.2,\n",
        "    callbacks=[print_callback]\n",
        ")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "4337/4337 [==============================] - ETA: 0s - loss: 1.9405\n",
            "==== Generating text after Epoch 0\n",
            "==== Generating with seed: \"se thee.   COSTARD. O, Marry me to one F\"\n",
            "se thee.   COSTARD. O, Marry me to one Freme quemin.—Sit wath misple, in thee herse of thou ind nown, sipapiend you to cincry ant thus coutias, And,  bath munse Nope-and praelycl..      BE NEN. AN, Gresender CaRsing of the PrRancurie _atwares   [Leapimy Eater did._] Kind I mone oun ow? Mionos thee and, Whiling fail must, Whow whink with.   CLOLWILES And a Sneened Chiles and Evening at in and Aid forthis Encille in ties gonaunce ot at li\n",
            "4337/4337 [==============================] - 47s 11ms/step - loss: 1.9405\n",
            "Epoch 2/10\n",
            "4333/4337 [============================>.] - ETA: 0s - loss: 1.8243\n",
            "==== Generating text after Epoch 1\n",
            "==== Generating with seed: \"ke no strain But that Achilles, were his\"\n",
            "ke no strain But that Achilles, were his be, not, apin brow wall not heartongely of yer. ALBETO. No treengging thot boer, think, muke my farson) Shall the priking the viveness a cullates.  VIVIA. Nay lord! Yee have her helf all when tham know she the eam, fire?   LORTUS. My amanitly, stave il jeevary: thal godge, an not more’st me. 5o hee; by nom, evessol, sparanys he more, From that not my fonas, And true heavin Talls too? If it say we\n",
            "4337/4337 [==============================] - 46s 11ms/step - loss: 1.8243\n",
            "Epoch 3/10\n",
            "4337/4337 [==============================] - ETA: 0s - loss: 1.7487\n",
            "==== Generating text after Epoch 2\n",
            "==== Generating with seed: \"ld stock, and freshly grow; then shall P\"\n",
            "ld stock, and freshly grow; then shall Partle low Glinding his touth-plies one fashorge; Un Orce an thy the vervys be wit brans and a   Asterrader the boding lelion. Speak come edsolk inquamange untanusen. There I gain Yefflacues so to que is here.     Theur to not saw Wholet for your and his mastenfultonles all Het inglifednt;               D4UT         MANGAR.      Come, then you, grading as. Cunofther praceious.  PALIA. Firf and I ne\n",
            "4337/4337 [==============================] - 46s 11ms/step - loss: 1.7487\n",
            "Epoch 4/10\n",
            "4333/4337 [============================>.] - ETA: 0s - loss: 1.6939\n",
            "==== Generating text after Epoch 3\n",
            "==== Generating with seed: \"pay you with unthankfulness in thought, \"\n",
            "pay you with unthankfulness in thought, I forlese arting gives of my Bratue cold.  SCENE. When every hand upandres, he wife-versear-     Mestrectay’s mineed-siny-to she cold the outh, The let my so.   LAALIO]  BAUS. You, whick, coprept Edv’d reray him. To never our come natuse beingne seenfecce, at so whendenn'st there the gratch’s Arinylus be, sould confecter of doe?     He ale;     And were spase bojelance worn, I have thee, but I mos\n",
            "4337/4337 [==============================] - 46s 11ms/step - loss: 1.6939\n",
            "Epoch 5/10\n",
            "4337/4337 [==============================] - ETA: 0s - loss: 1.6523\n",
            "==== Generating text after Epoch 4\n",
            "==== Generating with seed: \"    Shall bear the business in some othe\"\n",
            "    Shall bear the business in some other de than them? Art hat wons men, he as last detreat.  GHORTHUS. A deanr that avow it his a counse for your ryss me, She had bray rame.     Her it iborthy wast straity and oull for here.     For nomes, beargs, that howshant the clousone;     O been in the wructing-your Tolds in heart, sterk to hear.     Give you ambyar.  SECNOR. Good now, the whise she sime of my laisure:     For most mey one spea\n",
            "4337/4337 [==============================] - 46s 11ms/step - loss: 1.6523\n",
            "Epoch 6/10\n",
            "4336/4337 [============================>.] - ETA: 0s - loss: 1.6198\n",
            "==== Generating text after Epoch 5\n",
            "==== Generating with seed: \"ith a derry, And a derry, and a downe, S\"\n",
            "ith a derry, And a derry, and a downe, Suw, a come? 'twill that wemple him God     At streath a.  SIM LARDERLEIA. There our heatch ignicoing Armethis strangion and I hand this deserbli! A jesifit; and Mest name When lide with loves! woid? My lord upbury, therefore they see the buth one for my fremeded frue his conjurce Volions on it.  FIRSBOR. I go—do re't Emblard in thir killy? Fallown fall, are exiry enrewh with feiriding latiritions,\n",
            "4337/4337 [==============================] - 46s 11ms/step - loss: 1.6198\n",
            "Epoch 7/10\n",
            "4335/4337 [============================>.] - ETA: 0s - loss: 1.5934\n",
            "==== Generating text after Epoch 6\n",
            "==== Generating with seed: \" form following. Now, sir, for the manne\"\n",
            " form following. Now, sir, for the manner time no honss.o they dost, it what art himse: Or to.  NOIDINGBESSER. Dink, heage think hap thee, saim                                     Enter LOWN    SOBETRA. Is sent this hath dons for due.  PERVIAN. My have: Madam, vouch-these very clace their swear Both did but seaz'd and that the wisted gensuat     indenies,     That thou prumfess my souns; so ’en I do wocksold, said all: phorse     Thie a\n",
            "4337/4337 [==============================] - 46s 11ms/step - loss: 1.5934\n",
            "Epoch 8/10\n",
            "4337/4337 [==============================] - ETA: 0s - loss: 1.5717\n",
            "==== Generating text after Epoch 7\n",
            "==== Generating with seed: \"shall be honours, though indeed     In a\"\n",
            "shall be honours, though indeed     In as the Pordese of well Majeward 'the Itume fore and the dayy, depinasmend much peorave.   HOBANY. Putal any’s goor two a feal us Treast tray, Should sir, sit to chus to keep; eptile things cound.         GERAT.      No mo, days; yet repies, it is the satis, leavicumes      him: roliem their for so lived him done heave: puspony vilia. The sevier! day infected inder choose in MARDER]  I God bet Lians\n",
            "4337/4337 [==============================] - 47s 11ms/step - loss: 1.5717\n",
            "Epoch 9/10\n",
            "4337/4337 [==============================] - ETA: 0s - loss: 1.5532\n",
            "==== Generating text after Epoch 8\n",
            "==== Generating with seed: \"Caesar? Art     thou led in triumph? Wha\"\n",
            "Caesar? Art     thou led in triumph? What's thinfer’s on; we sleess, chatch. There’s graling oble truivas of forewell-kill Kun, good good,     And cause paint in there.   CLIVA. We wife I jult the maith? I cit it in heavenly preteris; to pliesty sweet the Maje me sporest the cirred. Then, more of thy matter to a melaible visores     Howhat shall be natusting are forth of more iscur.   THIRNY. What camesty solds thus other the caws no me\n",
            "4337/4337 [==============================] - 47s 11ms/step - loss: 1.5532\n",
            "Epoch 10/10\n",
            "4333/4337 [============================>.] - ETA: 0s - loss: 1.5372\n",
            "==== Generating text after Epoch 9\n",
            "==== Generating with seed: \"ess, Marcius,     Their bands i' th' vaw\"\n",
            "ess, Marcius,     Their bands i' th' vawicain by thee a truer his fam.  CAUELLE. Whose on good object, that he ray, xempley, my house.  Who, thou dinder’d her judished rathent to have me.                                          Exit sar               Exeunt ARCOND    My braving heeching are you he bearsore the followery.   [_Exeunt, ESpusing. KING RICHARD FRANCK.       Covisty netch mettight his words     I gave your chearch peep to ge\n",
            "4337/4337 [==============================] - 46s 11ms/step - loss: 1.5371\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3b7d58b240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1YkmCvA9iVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = model.predict(X[[0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BKrmPc6A5m3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "9e4acfd8-73ba-4ba1-f898-84053198a0d7"
      },
      "source": [
        "pred[0]"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4.28326275e-05, 1.11069879e-03, 1.73306209e-03, 2.63820084e-06,\n",
              "       1.47657311e-05, 4.34391259e-04, 3.64584941e-03, 4.02148766e-03,\n",
              "       4.07605711e-03, 6.88358676e-04, 2.59594731e-06, 4.75905603e-04,\n",
              "       1.25139747e-02, 2.57493026e-04, 2.81900610e-03, 1.98513753e-05,\n",
              "       1.68040697e-03, 4.84629010e-04, 8.37468033e-05, 5.17744114e-09,\n",
              "       2.35273107e-03, 7.95037746e-02, 2.24698684e-03, 9.94355767e-04,\n",
              "       4.21971072e-06, 9.18396469e-03, 3.52732809e-06, 1.67343984e-07,\n",
              "       3.22475610e-03, 5.46023576e-03, 2.80694053e-07, 2.76742945e-03,\n",
              "       1.10706512e-03, 5.94337878e-04, 1.38332474e-03, 1.50723890e-05,\n",
              "       1.40917522e-03, 4.04226239e-06, 4.07532061e-04, 8.91561271e-04,\n",
              "       7.21118195e-05, 5.10165701e-05, 4.95659291e-09, 5.69546901e-05,\n",
              "       4.06187406e-04, 3.50763514e-07, 6.13248408e-01, 1.40753221e-02,\n",
              "       1.34512593e-04, 1.41478807e-01, 5.37898450e-04, 1.82140575e-05,\n",
              "       3.41415126e-03, 1.36435870e-02, 1.25670352e-03, 3.68009169e-05,\n",
              "       1.05813807e-02, 1.60189043e-03, 1.79229144e-04, 4.57787610e-06,\n",
              "       8.40110151e-05, 5.13544364e-06, 4.22175113e-08, 2.44431186e-09,\n",
              "       2.21971911e-03, 4.50198598e-07, 2.25186595e-05, 3.23601967e-09,\n",
              "       1.43604074e-02, 5.46859313e-09, 1.74177755e-02, 5.47813306e-06,\n",
              "       4.61189169e-03, 1.27498305e-08, 5.05708298e-03, 3.71592469e-04,\n",
              "       1.68496604e-06, 6.20790815e-05, 1.73015051e-05, 2.16424419e-03,\n",
              "       1.74803936e-05, 1.58664386e-06, 3.62160517e-06, 1.80414554e-05,\n",
              "       4.25003491e-05, 2.67048337e-04, 4.47410908e-09, 1.28269661e-03,\n",
              "       1.20740791e-03, 4.12890571e-04, 2.23008115e-04, 2.81899411e-04,\n",
              "       4.45774465e-04, 1.41955505e-04, 1.86270611e-07, 6.52889032e-09,\n",
              "       6.89836976e-04, 1.57412444e-07, 3.00307347e-05, 2.07200134e-03],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxWqUJ0_EomA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d01d0280-c817-47c4-8ba5-8fe94541e251"
      },
      "source": [
        "pred[0].sum()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tn4TkzTCSsT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def joust(integer):\n",
        "  start_index = random.randint(0, len(spearshake0) - maxlen - 1)\n",
        "  generated = ''\n",
        "  sentence = spearshake0[start_index: start_index+maxlen]\n",
        "  seed_text = 'Seed text: \"'+ sentence +'\"'\n",
        "  # sys.stdout.write(generated)\n",
        "  for i in range(integer):\n",
        "    X_pred = np.zeros((1, maxlen, len(chars)))\n",
        "    for t, char in enumerate(sentence):\n",
        "      X_pred[0, t, char_int[char]] = 1\n",
        "    preds = model.predict(X_pred, verbose=0)[0]\n",
        "    next_index = sample(preds)\n",
        "    next_char = int_char[next_index]\n",
        "    generated += next_char\n",
        "    sentence = sentence[1:]+next_char\n",
        "  return print(seed_text, '\\n', 'Shaken Speare:', generated)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDzB5oXdIb03",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c581aa9c-59dc-4206-a891-22abbeb2ad4c"
      },
      "source": [
        "joust(40)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed text: \"s set our men in order,     And issue fo\" \n",
            " Shaken Speare: r tooss did love you. Here is before the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFn7tO3bIiju",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "e2f51b29-92dc-49cf-a14e-35c60e8a6b9e"
      },
      "source": [
        "joust(400)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed text: \"and trembled at the ill neighbourhood.  \" \n",
            " Shaken Speare:  CLEOPATRIUS. In merite.     A beging end that so hear     Sirtain since word not sir of the Pance,     and Grief, thy honour King.  MERNA. I’en him and the leavence sud Worvey an back     Thou his write me.   FORD. Now! Ludied, then you shall issoment the speerer, And raugh to sweete ourgetice subland,     And pales ware’d the fastes leave For to thy     A will sloens thought me the speed and  To\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRdv7XXI0lNA",
        "colab_type": "text"
      },
      "source": [
        "This text file contains the complete works of Shakespeare: https://www.gutenberg.org/files/100/100-0.txt\n",
        "\n",
        "- [x] Use it as training data for an RNN - you can keep it simple and train character level, and that is suggested as an initial approach.\n",
        "\n",
        "- [x] Then, use that trained RNN to generate Shakespearean-ish text.\n",
        "\n",
        "- [x] Your goal - a function that can take, as an argument, the size of text (e.g. number of characters or lines) to generate, and returns generated text of that size.\n",
        "\n",
        "- [x] Note - Shakespeare wrote an awful lot. It's OK, especially initially, to sample/use smaller data and parameters, so you can have a tighter feedback loop when you're trying to get things running.\n",
        "\n",
        "- [x] Then, once you've got a proof of concept - start pushing it more!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zE4a4O7Bp5x1"
      },
      "source": [
        "# Resources and Stretch Goals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uT3UV3gap9H6"
      },
      "source": [
        "## Stretch goals:\n",
        "- Refine the training and generation of text to be able to ask for different genres/styles of Shakespearean text (e.g. plays versus sonnets)\n",
        "- Train a classification model that takes text and returns which work of Shakespeare it is most likely to be from\n",
        "- Make it more performant! Many possible routes here - lean on Keras, optimize the code, and/or use more resources (AWS, etc.)\n",
        "- Revisit the news example from class, and improve it - use categories or tags to refine the model/generation, or train a news classifier\n",
        "- Run on bigger, better data\n",
        "\n",
        "## Resources:\n",
        "- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) - a seminal writeup demonstrating a simple but effective character-level NLP RNN\n",
        "- [Simple NumPy implementation of RNN](https://github.com/JY-Yoon/RNN-Implementation-using-NumPy/blob/master/RNN%20Implementation%20using%20NumPy.ipynb) - Python 3 version of the code from \"Unreasonable Effectiveness\"\n",
        "- [TensorFlow RNN Tutorial](https://github.com/tensorflow/models/tree/master/tutorials/rnn) - code for training a RNN on the Penn Tree Bank language dataset\n",
        "- [4 part tutorial on RNN](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/) - relates RNN to the vanishing gradient problem, and provides example implementation\n",
        "- [RNN training tips and tricks](https://github.com/karpathy/char-rnn#tips-and-tricks) - some rules of thumb for parameterizing and training your RNN"
      ]
    }
  ]
}